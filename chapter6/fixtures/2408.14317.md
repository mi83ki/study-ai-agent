Title: 

URL Source: https://arxiv.org/pdf/2408.14317

Markdown Content:
# Claim Verification in the Age of Large Language Models: A Survey 

# Alphaeus Dmonte 1, Roland Oruche 2, Marcos Zampieri 1, Prasad Calyam 2, Isabelle Augenstein 3

> 1

George Mason University, USA 

> 2

University of Missouri-Columbia, USA 

> 3

University of Copenhagen, Denmark 

admonte@gmu.edu 

Abstract 

The large and ever-increasing amount of data available on the Internet coupled with the laborious task of manual claim and fact verification has sparked the interest in the develop-ment of automated claim verification systems. 1 Several deep learning and transformer-based models have been proposed for this task over the years. With the introduction of Large Language Models (LLMs) and their superior performance in several NLP tasks, we have seen a surge of LLM-based ap-proaches to claim verification along with the use of novel methods such as Retrieval Augmented Generation (RAG). In this survey, we present a comprehensive account of recent claim verification frameworks using LLMs. We describe the different components of the claim verification pipeline used in these frameworks in detail including common approaches to retrieval, prompting, and fine-tuning. Finally, we describe publicly available English datasets created for this task. 

# Introduction 

False information is widely present on social media and on the Web motivating the development of automated fact ver-ification systems (Guo, Schlichtkrull, and Vlachos 2022). The introduction of LLMs has provided malicious actors with sophisticated ways of creating and disseminating false information. Recent election cycles saw a large number of claims spread across both social media and news platforms alike (Dmonte et al. 2024). Similarly, during the COVID-19 pandemic, many claims were spread across social media platforms. Many of these claims were factually inaccurate which led to the spread of misinformation (Zhou et al. 2023). Fact-checking potentially false claims is an essential mod-eration task to reduce the spread of misinformation. Several organizations such as FactCheck, PolitiFact, NewsGuard, and Full Fact perform manual fact-checking to verify claims in different domains. However, this has been regarded as a laborious task that requires domain expertise (Adair et al. 2017; Hanselowski 2020) and it has become less and less feasible due to the sheer volume of misinformation that can be generated by humans and AI models. Automated fact-checking has become an increasingly popular approach to 

> 1

We use the terms claim verification and fact verification inter-changeably given the overlap between the two concepts. Claim: “Homer Simpson is a cartoon character on Seinfeld.”          

> Claim Claim
> Retriever
> Module
> Retriever
> Module
> Rationale
> Selection
> Top KDocs
> …
> Prompt
> Claim Verdict
> “False. Homer Simpson is a
> character on The Simpsons. ”
> Label Evidence
> LLM
> Top KDocs
> …
> Doc1: <Text> …
> State whether claim is
> true or false.
> C: “Homer Simpson…
> Reason Fine -Tune
> False
> a) Traditional NLP b) LLM -based
> Database
> ICL

Figure 1: Comparison of claim verification systems between NLP-based (traditional) and LLM-based for claim veracity. verify the veracity of claims in a given text. 2 There are sev-eral steps involved in the fact-verification pipelines, but the three main components are claim detection, evidence re-trieval, and veracity prediction. Models used in these steps have followed the general methodological developments of the field and we have thus observed an increase in the use of LLMs for claim verification (Zhang and Gao 2023; Wang and Shu 2023; Quelle and Bovet 2024). Figure 1 shows an example claim veracity scenario and compares the architec-tural differences between NLP-based (traditional) and LLM-based claim verification systems. Compared to traditional systems which use NLP-based models, fact verification are less prone to error propagation, and provide explainable/in-

> 2

We acknowledge that claim verification models can also be applied to other modalities of data (e.g., images). In this survey, we address models applied to text only.    

> arXiv:2408.14317v1 [cs.CL] 26 Aug 2024 Claim Detection Claim Check-
> worthiness
> Claim Matching
> Evidence Retrieval Rationale Selection
> Veracity Prediction Explanation
> Generation
> Verified Claims
> External Data/ Resources

+Figure 2: The archetypal claim verification pipeline and its main components. terpretable generations/justification when verifying claims. Despite this, LLMs are pre-trained on very large collec-tions of texts and are prone to hallucinations, often gener-ating texts containing incorrect information. Furthermore, such models can be used to generate misinformation at scale (Chen and Shu 2023; Zhou et al. 2023; Dmonte et al. 2024) and therefore they can be exploited by malicious actors to generate and spread wrong and factually incorrect informa-tion at an unprecedented rate (Pan et al. 2023c). Further-more, using these pre-trained models for fact-verification may generate incorrect veracity labels, as the models may also rely on obsolete information to assess the veracity of a claim. Approaches like RAG (Gao et al. 2023), which aid the models in their decision-making abilities are used in this task for the model to get access to the most recent informa-tion during the fact-verification task. A few general automated claim verification surveys have been published over the years (Zeng, Abumansour, and Zu-biaga 2021; Bekoulis, Papagiannopoulou, and Deligiannis 2021; Guo, Schlichtkrull, and Vlachos 2022) as well as a couple of surveys focusing on particular aspects of the task such as explainability (Vallayil et al. 2023) and applica-tions to specialized domains such as scientific texts (Vladika and Matthes 2023). However, all past related surveys, lack consideration for LLM-based approaches or focus on spe-cific sub-tasks of the pipeline (Panchendrarajan and Zubi-aga 2024). In this paper, we fill this important gap by sur-veying LLM-based frameworks proposed in recent years. To the best of our knowledge, this is the first survey to explore claim verification with LLMs. We expect it to be a valuable resource to researchers in the field opening exciting new av-enues for future research. 

# Search Criteria 

We search various well-known repositories 3 of scientific ar-ticles to collect the papers that serve as primary sources           

> 3ACM: portal.acm.org . IEEE Xplore: ieeexplore.ieee.org . Sco-pus: scopus.com . ACL: aclanthology.org . Web of Science: isi-knowledge.com .Springer: link.springer.com .ArXiv: arxiv.org .CEUR: https://ceur-ws.org/ .

for this survey. We apply queries including LLMs , claim verification , fact verification and related keywords on these repositories. We focus primarily on the ACL Anthology , the 

ACM Digital Library , and IEEE Xplore , and proceedings of related conferences such as AAAI and IJCAI . We further search on Scopus , Springer Link , and Science@Direct , and 

ArXiv .We collected and reviewed over 100 papers based on our search terms and filter papers based on the following search criteria: (i) since the scope of our review is on text-based LLM claim verification systems, we omit papers re-lated to multimodal approaches (e.g., text-to-images), our papers from other modalities in claim verification (e.g., im-ages, graph-based systems). (ii) since our review focuses on LLM-based systems for claim veracity, we omit papers re-lated to claim identification, claim detection, and/or detect-ing LLM-generated content. Based on our filtering criteria, we collected a total of 49 papers related to LLM-based ap-proaches for veracity labeling. The papers on topic LLM-based veracity labeling have been published primarily in the ACL Anthology and ACM Digital Library, but also in other repositories such as the IEEE Xplore. We release this curated list to be publicly available. 

# Claim Verification Pipeline 

Figure 2 shows an archetypal claim verification pipeline consisting of the following modules: claim detection, claim matching, claim check-worthiness, document/evidence re-trieval, rationale/sentence selection, veracity label predic-tion, and explanation/justification generation. Many pro-posed systems make use of only some of these modules. The following subsections describe each of these modules in de-tail. Finally, as described in Panchendrarajan and Zubiaga (2024), performance evaluation in these sub-tasks is usually carried out using well-established automatic evaluation met-rics used in text classification such as Precision, Recall, and F-score. 

Claim Detection Input texts may contain one or more statements but not all statements are claims. Given the input text, a claim detection module is designed to identify all the Input Claim Evidence Retrieval   

> LLM Generatio n
> Prompt Creation
> Label
> Generation
> Evidence
> Generation
> Explainable
> Generation
> Claim Detection Claim Check-
> worthiness
> Claim Matching
> Evidence Retrieval
> Rationale Selection
> Veracity Prediction
> Explanation
> Generation
> Verified Claims
> External Data/ Resources

+ 

> External Data/ Resources
> Claim Detection Claim Check-
> worthiness
> Claim Matching
> Evidence Retrieval
> Rationale Selection
> Veracity Prediction
> Explanation
> Generation
> Verified Claims
> External Data/ Resources
> +
> Transfer Learning
> Fine -Tuning
> In -Context
> Learning

Figure 3: Claim verification pipeline using large language models. Instead of traditional automatic claim verification, this pipeline creates a prompt from the retrieved evidence and the input claim as input to the LLM to generate a label, sentence evidence, and/or explanation of its response. statements containing a claim. For example, the statement 

’I loved the movie Oppenheimer.’ is an opinion, whereas the statement ’The COVID-19 pandemic started in Texas.’ con-tains a claim. 

Check-worthy Claim Identification Not all the identified claims are check-worthy. In the check-worthy claim identi-fication sub-task, given an input claim, the task is to identify the claims that include real-world assertions and that may need to be verified (Hassan, Li, and Tremayne 2015; Nakov et al. 2021). This is a subjective task and it relies on the factors like popularity of the claim, public interest in deter-mining the veracity of the claim, etc. For example, the claim 

’The President met the State Governor to discuss the infras-tructure deal.’ is less check-worthy than the claim ’Drinking salt water cures COVID.’ .

Claim Matching The identified check-worthy claims can be matched to the previously fact-checked claims. Given an input claim and a database of previously fact-checked claims, claim matching is used to determine if the input claim is previously fact-checked and exists in the database (Shaar et al. 2020; Nakov et al. 2021). This can help avoid the next steps in the pipeline and a veracity label can be pre-dicted directly. 

Document/Evidence Retrieval If the input claim does not exist in the database of fact-checked claims, the claim needs to be verified. In the document or evidence retrieval sub-task, all the relevant documents related to the input claim are extracted, either from an external database or through an internet search/information retrieval(Chen et al. 2017). A threshold of how many documents are to be retrieved can be predetermined. 

Rationale/Sentence Selection All the information in the retrieved documents is not relevant to the claim. Hence in the rationale selection task, only the information or evidence most relevant to the claim are selected to be used to predict the veracity label. 

Veracity Label Prediction Once the rationale is selected, the claim along with the rationale and additional features if any, are given as input to a machine learning model. The task of the classifier is to predict a veracity label from among the following three labels; ’SUPPORTED’, ’REFUTED’, or ’NOT ENOUGH EVIDENCE’. In some datasets, the labels can also be ’TRUE’, or ’FALSE’, depending on the specific task. 

Explanation/Justification Generation Recent works have focused on generating explanations for the veracity labels prediction. This specific task is focused on generating natural language justifications or explanations for the prediction considering the claim and evidence to generate these explanations. 

# LLM Approaches 

In this section, we describe the recent advancements that en-able LLMs to be robust in fact verification scenarios. Fig-ure 3 shows an example pipeline that encapsulates multi-ple component modules (i.e., Evidence Retrieval, Prompt Creation, Transfer Learning, and LLM Generation) for ver-ifying claims. Different from traditional fact verification pipelines that select evidence set for verifying claims, LLM-based claim verification conditions generated text based on the concatenated input claim and retrieved evidence. The ability of large, pre-trained LMs augmented with retrieval enables them to perform well on knowledge-intensive tasks such as text generation. 

Evidence Retrieval Strategies 

RAG models, which have been developed to combat the is-sue of hallucination in LLMs in knowledge-intensive tasks, have shown success in the scope of fact verification (Lewis et al. 2020; Izacard et al. 2023; Gao et al. 2023; Guan et al. 2023). Early work such as in Lewis et al. (2020) developed a framework that incorporates a retriever to retrieve evidence from an external database such as Wikipedia for condition-ally generating veracity labels in fact verification. Authors in Izacard et al. (2023) demonstrate that RAGs perform well on the FEVER shared task (Thorne et al. 2018b)) in few-shot settings, showing approximately 5% improvement over large-scale LLMs such as Gopher (Rae et al. 2021) with sig-nificantly fewer parameters (i.e., 11B compared to 280B). Other works consider the optimization of either document ranking (Glass et al. 2022; Chen et al. 2022c; Hofst¨ atter et al. 2023) or input claims (i.e., queries) (Hang, Yu, and Tan 2024) as a crucial step for improving evidence retrieval for veracity labeling. Authors in Hofst¨ atter et al. (2023) use an autoregressive re-ranker to get the most relevant passages from the retriever. These are then passed to the generation model to generate the veracity label. Despite this, RAG models can often fail when encoun-tering long or complex input claims, causing the model to incorrectly generate veracity labels or evidence sentences. Recent works have addressed this issue by segmenting long claims into smaller sub-claims and performing multiple rounds of retrieval (Khattab, Potts, and Zaharia 2021; Shao et al. 2023). Authors in Khattab, Potts, and Zaharia (2021) present a pipeline for multi-hop claim verification that uses an iterative retriever and neural methods for effective doc-ument retrieval and re-ranking. Shao et al. (2023) show that using a re-ranker to distill knowledge to a retriever helps close the semantic gaps between a query and docu-ment passage when verifying claims using iterative RAGs. Other works address the issue of complex claims by using fine-grained retrieval techniques based on claim decompo-sition (Chen et al. 2023a; Zhang and Gao 2023; Pan et al. 2023b).Chen et al. (2023a) generate sub-questions based on a claim, which a document retriever uses to retrieve rele-vant documents. A fine-grained retriever retrieves top-k text spans as evidence based on a k-word window and BM25. Authors in Zhang and Gao (2023) decompose a claim into sub-claims and generate questions to verify the sub-claims. External knowledge sources are used to retrieve relevant in-formation to verify the sub-claims and generate a final verac-ity label. Pan et al. (2023b) follow a programming paradigm, where the claim is broken down into subtasks and the final label is the aggregation of the execution of each subtask. The fact-verification subtask uses external knowledge to retrieve relevant evidence for a claim. Hang, Yu, and Tan (2024) retrieve evidence based on generated knowledge graphs of evidences. They generate a knowledge graph of user query or input claim and com-pare it to the database of knowledge graphs to retrieve the most relevant information for claim verification. Authors in Hu et al. (2023) propose a latent variable model that al-lows the retrieval of the most relevant evidence sentences from a document while removing the irrelevant sentences. This approach reduces the noisy data during the verification process. Stochastic-RAG, an approach proposed by Zamani and Bendersky (2024) uses a stochastic sampling without re-placement process for evidence retrieval and selection. This approach overcomes the ranking and selection of the evi-dence hence optimizing the RAG model. Authors in Zhang et al. (2023) optimize the evidence retrieval process by using feedback from the claim verifier. The divergence between the evidence from a retrieved evidence set provided to the verifier and the gold standard evidence, acts as a feedback signal used to train the retriever. Xu et al. (2024b) propose Search-in-the-Chain, an approach where an LLM generates a reasoning chain and based on the answer to each node in the chain, retrieval can be used to correct an answer or pro-vide additional knowledge. The approach improves the gen-eration accuracy of the LLM. 

Prompt Creation Strategies 

Text prompting has been shown to be an effective technique for improving the desired output of large-scale generative models. In the context of claim verification, several works investigate prompting strategies, using both manual and au-tomated techniques, for building more robust claim verifica-tion systems Zhang and Gao (2023); Li et al. (2023c); Zeng and Gao (2023); Chen et al. (2023b). Authors in (Zhang and Gao 2023) develop a hierarchical prompting technique that enables LLMs to verify multiple sub-claims using a step-by-step approach. The work in Li et al. (2023c) demonstrates a self-sufficient claim verification through prompting instruc-tions on multiple language models. ProToCo (Zeng and Gao 2023) demonstrates improved claim verification performance of LLMs by leveraging a consistency mechanism to construct variants of the original claim-evidence pair prompt based on three logical relations (i.e., confirmation, negation, uncertainty). Authors in Chen et al. (2023b) develop a unified retrieval framework that em-ploys discrete, continuous, and hybrid prompt strategies for adjusting to various knowledge-intensive tasks such as claim verification. Other works such as the FactualityPrompts (Lee et al. 2022) framework test the output generations of LLMs given an input prompt and use an external database such as Wikipedia to calculate factuality and quality measures com-pared to the ground truth. Other works aim to improve the LLM’s reasoning abilities by appending the claims with ev-idence during prompting in the context of claim verifica-tion and text generation (Parvez 2024; Dougrez-Lewis et al. 2024). 

Transfer Learning Strategies 

Fine-Tuning. Although recent studies show the success of pre-trained LLMs on zero- or few-shot tasks, they of-ten fail to verify real-world claims given their limited in-ternal knowledge. The success of fine-tuning has motivated recent work on claim verification (Chen et al. 2022b; Pan et al. 2021; Zeng and Zubiaga 2024). The work in Chen et al. (2022b) leverages a language model to fine-tune over an ex-ternal corpus for retrieving passage titles and evidence sen-tences using constrained beam search. The results showed improved performance against the traditional fact verifica-tion pipeline over the FEVER dataset. Other work demon-strates that using GPT models to generate synthetic training data improves the performance of LLMs on various tasks such as fact checking (Tang, Laban, and Durrett 2024) and claim matching (Choi and Ferrara 2024). Authors in Pan et al. (2021) develop a pipeline for cre-ating a fact verification dataset and fine-tuning a language model by leveraging passages from Wikipedia to generate QA pairs related to claim veracity. The authors show that it can improve state-of-the-art language models in zero-shot settings. The work in (Zeng and Zubiaga 2024) shows that using unlabelled pairwise data to increase the alignment be-tween claim-evidence pairs shows significant improvement of LLM performance in few-shot claim verification tasks. In addition, other recent works leverage techniques such as re-inforcement learning to fine-tune models for improving the veracity of claims and supporting evidence Zhang and Gao (2024); Huang et al. (2024). A document-level and question-level retrieval policy is proposed by Zhang and Gao (2024), where the top-k and top-1 documents for the document and question-level policy respectively are used as input to a scoring function for label prediction during training. This approach outperforms retrieval and prompting approaches. Chiang et al. (2024) fine-tune LLMs for their multi-stage fact verification approach. They fine-tune a model to gen-erate answers based on claim-evidence pairs and a set of questions, whereas another model is fine-tuned to verify the claim based on the claim-evidence and question-answer pairs. Authors in Zhu et al. (2023) fine-tune a generation model to generate counterfactuals to train the fact verifica-tion model’s performance on out-of-domain claims. 

In-Context Learning. The recent success in the perfor-mance of pre-trained LLMs in zero- and few-shot settings is largely attributed to its in-context learning (ICL) abili-ties (Kojima et al. 2022; Brown et al. 2020). In the scope of claim verification, popular ICL techniques with strong zero-and few-shot performance include chain-of-thought (CoT) reasoning (Wei et al. 2022). The work in Zhao et al. (2024) develops a multi-stage verification pipeline for claim veri-fication based on claim decomposition and self-reflection. An LLM-based verifier module is created using instruc-tion prompting to generate a reasoning analysis among all sub-claims created by the decomposer module. The results suggest that using zero-shot prompting techniques provides better performance in multi-hop claim verification domains such as HOVER and FEVEROUS compared to few-shot prompting and fine-tuning methods. Authors in Kanaani (2024) enable LLMs to generate reasons over retrieved ev-idence in claim verification using few-shot ICL and the STaR CoT technique inspired by the work of Zelikman et al. (2022). Similar work has leveraged CoT techniques for the purposes of effectively verifying complex claims using rea-soning steps (Yao et al. 2023; Ni et al. 2024). On the other hand, HiSS (Zhang and Gao 2023) demonstrates that using prompting techniques for few-shot learning and claim de-composition can substantially improve the performance of CoT models for complex news claim verification. Li et al. (2023b) leverage the ICL capability of LLMs to perform multiple tasks simultaneously. Their approach outperforms or achieves comparable task performance in a zero-shot set-ting on claim verification datasets. 

LLM Generation Strategies 

Label and Evidence Generation. While the majority of claim verification systems predict veracity labels based on the concatenation of the input claim and the set of evidence sentences (Pradeep et al. 2021), recent work has proposed alternate strategies for determining veracity labels as well as selecting/generating evidence pieces. The work in Cao et al. (2024) develops, SERIf, a claim verification pipeline that features an inference module that predicts the veracity label of scientific news articles based on a two-step summariza-tion (i.e., ‘Extractive - Abstractive‘) and evidence retrieval technique. Each summary-evidence pair is fed into the LLM and it produces a binary label, indicating whether the news article is reliable (supported) or unreliable (refuted). Authors in Wadden et al. (2022b) leverage the Longformer trans-former model (Beltagy, Peters, and Cohan 2020) that uses a shared encoding over the claim and document abstracts for rationale identification and claim label prediction. The work in Li et al. (2024a) develops an algorithm that selects the minimal evidence group (MEG) within a set of retrieved candidate documents. The algorithm aims to min-imize the redundancy while also selecting the most relevant piece of evidence to prompt the language model. Authors in Chen et al. (2022b) create an LLM claim verification framework in which the authors use the BART model to en-code all candidate sentences from the most relevant retrieved documents. The BART decoder serves as an evidence de-coder to predict the g-th evidence sentence via generation conditioned on the top k retrieved documents and the input claim. Lee et al. (2022) developed a variant of nucleus sam-pling called, factual-nucleus sampling , in which the top-p

sampling pool is selected as a set of sub-words whose cumu-lative probability exceeds p. The authors show that factual-nucleus sampling can improve evidence and label genera-tion without claim verification datasets such as FEVER. Kao and Yen (2024) propose a multi-stage approach, where the evidence sentences are retrieved from articles related to a claim, and arguments are generated by aggregating and re-constructing the evidence. The arguments are refined and passed to a LLM to generate a verification label. Other ap-proaches leverage LLMs reasoning capabilities to generate veracity labels and factual evidence (Cheng, Tan, and Lu 2024; Jafari and Allan 2024; Li et al. 2024b; Fang et al. 2024; Pan et al. 2023a). 

Explainable Generation. Recent studies investigate ex-plainable approaches to improve LLM-based claim verifi-cation systems (Wang and Shu 2023; Dammu et al. 2024; Si et al. 2023). Authors in Wang and Shu (2023) present an ex-plainable claim verification framework named FOLK, that leverages the explanation capabilities of LLMs when verify-ing a claim and justifies the prediction through a summary of its decision process. The work in Dammu et al. (2024) proposes a knowledge graph (KG)-based approach for text verification and evidence attribution. An objective function is used to fine-tune LLMs on evidence attribution based on the input text and retrieved triplets from KG, inducing expla-nations on claim predictions. While explainable techniques can aid humans in fact-checking, LLMs are prone to incor-rect explanations due to hallucinations, causing them to be unreliable in certain claim verification scenarios (Si et al. 2023). Ma et al. (2024) prompt an LLM in a few-shot set-ting to generate a concise summary of the gathered evidence documents and the input claim. This summary serves as an explanation for the verified claim. Other works in claim verification leverage reasoning techniques such as chain-of-thought (CoT) for enabling the LLM to be interpretable in its decision-making process when verifying claims (Yao et al. 2023; Pan et al. 2023a; Zhao et al. 2024; Kanaani 2024; Ni et al. 2024; Quelle and Bovet 2024; Fang et al. 2024). The authors in Pan et al. (2023a) and Fang et al. (2024) leverage Dataset # Instances Source # Labels Reference                                                                                                                             

> AVeriTeC 4,568 Google FactCheck Claim Search API 4Schlichtkrull, Guo, and Vlachos (2024) BINGCHECK 3,840 Microsoft Copilot 4Li et al. (2023c) Check-COVID 1,504 News websites, CORD-19 3Wang et al. (2023a) CLAIMDECOMP 1,000 PolitiFact 6Chen et al. (2022a) CLIMATE-FEVER 1,535 Google Search, Wikipedia 3Diggelmann et al. (2020) CoVERT 212 Manually created 3Liu et al. (2024) COVID-Fact 4,086 Reddit 2Saakyan, Chakrabarty, and Muresan (2021) EX-FEVER 60,000 Wikipedia 3Ma et al. (2024) ExpertQA 2177 LLM, Google Search 5Malaviya et al. (2024) FACTIFY-5WQA 391,041 Public Fact-Verification Datasets 3Rani et al. (2023) FactCheck-Bench 678 LLM 4Wang et al. (2023b) FactCheckQA 20,871 PolitiFact 3Bashlovkina et al. (2023) FACTUALITYPROMPTS 16,000 Wikipedia, LLM 2Lee et al. (2022) FAVIQ 188,000 Wikipedia, Existing Datasets 2Park et al. (2022) FEVER 185,445 Wikipedia 3Thorne et al. (2018a) FEVEROUS 87,026 Wikipedia 3Aly et al. (2021) FoolMeTwice 12,968 Wikipedia 2Eisenschlos et al. (2021) HealthVer 14,330 Bing Web Search API 3Sarrouti et al. (2021) HOVER 26,171 Wikipedia 2Jiang et al. (2020) LIAR 12,500 PolitiFact 6Wang (2017) MultiFC 34,918 Fact-checking Websites 2-40 Augenstein et al. (2019) PolitiFact 21,152 PolitiFact 6Misra (2022) QuanTemp 15,514 Google Fact Check Tool API 3Venktesh et al. (2024) RAWFC 2,012 Snopes 6Yang et al. (2022) SciFact 1,400 S2ORC 2Wadden et al. (2020) SciFact-Open 279 S2ORC 3Wadden et al. (2022a) SciNews 2,400 Wiki Fake News, LLM, Existing Datasets 2Cao et al. (2024) SCITAB 1,225 SciGen Dataset (arXiv) 3Lu et al. (2023) VitaminC 400,000 Wikipedia 3Schuster, Fisch, and Barzilay (2021) WiCE 1,967 Wikipedia 3Kamoi et al. (2023) XClaimCheck 16,177 PolitiFact 5Kao and Yen (2024)

Table 1: Available datasets along with the number of instances, data sources, and references. the reasoning ability of the LLMs to generate explanations by using question-guided reasoning and minimizing the in-herent model biases respectively. 

# Evaluation and Benchmarking 

Metrics 

The F1 score is the most commonly used metric to mea-sure the performance of automatic claim verification sys-tems. Other metrics like Precision, Recall, and Accuracy are also used to evaluate the system performance. Katrani-dis and Barany (2024) used the error rate between the hu-man and automated fact-verification system to measure the verification accuracy. However, these metrics consider a single pipeline component to evaluate the system’s overall performance. Hence, Thorne et al. (2018a) introduced the FEVER score, a metric that uses both the verification ac-curacy as well as the evidence retrieval accuracy to com-pute overall system performance. While these metrics are valuable for assessing the performance of the classifica-tion tasks, they are inadequate for evaluating the perfor-mance of the non-classification components of the pipeline. Hence, metrics like Recall@k are used to measure the per-formance of the retrieval task (Pan et al. 2023b; Pradeep et al. 2021), while BLEU and METEOR are used to eval-uate the quality of explanations or generated questions and answers. Schlichtkrull, Guo, and Vlachos (2024) propose a new evaluation metric AVeriTeC score that uses METEOR and accuracy metrics, for question-answer-based veracity la-bel prediction systems. Other metrics like Mean Absolute Error (MAE), Expected Calibration Error (ECE), Area Un-der ROC Curve (AUC-ROC), and Pearson’s Correlation are also used. Most of these metrics are inadequate to evalu-ate the factual accuracy of the LLM-generated text. Sev-eral metrics like FactScore (Min et al. 2023), SAFE (Wei et al. 2024), and VERISCORE (Song, Kim, and Iyyer 2024) are introduced to evaluate the factual accuracy of the LLM-generated text. While these metrics evaluate the accuracy, other metrics and frameworks to evaluate the factual errors in generated text (Lee et al. 2022; Chern et al. 2023) as well as their alignment (Zha et al. 2023) and entailment (Lee et al. 2022) considering the factuality have been proposed. 

Datasets 

The fundamental resource for training and evaluating claim verification systems is datasets containing annotated texts. As most research in this area deals with English data, we col-lect information about all publicly available English datasets used in the papers discussed in this survey and present them in Table 1. Several general-domain datasets have been released over the years. Different online data sources and websites are used to extract facts and claims including Wikipedia (Thorne et al. 2018a; Jiang et al. 2020; Diggelmann et al. 2020; Eisenschlos et al. 2021; Schuster, Fisch, and Barzilay 2021; Kamoi et al. 2023), due to the extensive amount of infor-mation available spanning various topics and domains, and online fact-checking websites like PolitiFact (Wang 2017; Augenstein et al. 2019; Kao and Yen 2024). Factually in-correct claims are shared through social media channels like X, Reddit, etc. Saakyan, Chakrabarty, and Muresan (2021) introduced the COVID-Fact dataset consisting of claims ex-tracted from Reddit posts. Several datasets for scientific fact verification have also been introduced over the years (Wad-den et al. 2020, 2022a; Lu et al. 2023). Most fact-verification datasets rely on unstructured textual evidence. Hence, a few datasets with structured data sources as evidence have been introduced (Aly et al. 2021; Lu et al. 2023). While a large number of datasets are focused on only veracity la-bels, some datasets were deloped to address explainability as well (Chen et al. 2022a; Yang et al. 2022; Ma et al. 2024; Rani et al. 2023; Schlichtkrull, Guo, and Vlachos 2024). Most of the claim-verification datasets include claims extracted from available information sources. LLMs are widely used to generate content, given their superior text-generation capabilities. These models often generate fac-tually incorrect information. Efforts to identify such non-factual text have been undertaken. However, existing claim-verification datasets can be inadequate for this task due to the linguistic variations between the human- and LLM-generated text. To overcome this issue, LLM-generated claim verification datasets have been introduced (Li et al. 2023c; Cao et al. 2024). While these datasets are used to evaluate the automatic claim verification systems, there is a need to evaluate the factual accuracy of the LLM-generated text. Hence, a few datasets to evaluate the LLM generation’s factual accuracy have been introduced (Lee et al. 2022; Wang et al. 2023b; Malaviya et al. 2024). 

Shared Tasks 

Shared tasks are competitions where participating teams develop systems to solve a task using a common bench-mark dataset. There have been multiple shared tasks on the claim and fact-checking organized over the years like the Fact Extraction and Verification (FEVER) (Thorne et al. 2018b), TabFact 4, CLEF 2020 CheckThat! (Barr´ on-Cedeno et al. 2020), SCIVER (Wadden and Lo 2021), SEM-TAB-FACT (Wang et al. 2021), FACTIFY-5WQA 5, and more re-cently AVeriTeC 6. While there have been various techniques like question-answer generation as a precursor task to label prediction as in the AVeriTeC shared task, all these shared tasks are centered on predicting a veracity label for a claim. Given the LLM’s tendency to hallucinate generating plau-sible yet factually inaccurate text, there is a need to orga-nize shared tasks to evaluate the factual accuracy of LLM-generated content. 

> 4https://competitions.codalab.org/competitions/21611
> 5https://defactify.com/factify3.html
> 6https://fever.ai/task.html

# Open Challenges 

Handling Irrelevant Context Retrieved evidence may be irrelevant, which is a challenge for LLMs, as they may not be trained to ignore such evidence. The lack of robustness to the noise can cause the LLM to produce misinformation and incorrect verification. Recent research on open-domain question answering shows that external knowledge relevant to the task can aid the model performance, however, irrele-vant context can also lead the model to make inaccurate pre-dictions (Petroni et al. 2020; Shi et al. 2023; Li et al. 2023a; Yu et al. 2023). For the fact-verification task, recent works have proposed techniques to identify the most relevant con-text while aiding the veracity label prediction and thus im-proving the overall system performance (Wang et al. 2023c; Yoran et al. 2024; Xia et al. 2024). However, more work on the topic is required to verify the effectiveness of this design approach across multiple fact verification domains. 

Handling Knowledge Conflicts Fact-verification ap-proaches reliance on retrieved evidence can cause knowl-edge conflicts in LLM-based approaches where retrieved ev-idence as the internal parameters of the pre-trained LLM may conflict with the external knowledge. This causes the LLM to ignore the retrieved evidence and produces hallu-cinations. Xu et al. (2024a) provides an in-depth analysis of this scenario. Works by (Li et al. 2023a; Neeman et al. 2023; Mallen et al. 2023; Longpre et al. 2021; Chen, Zhang, and Choi 2022) introduce approaches to avoid knowledge conflicts and mitigate hallucinations for question-answering. Expanding this work to fact-verification, especially to the approaches that use LLMs, is vital for an effective verifica-tion process. 

Multilinguality Most automated claim verification ap-proaches rely on English datasets. Furthermore, there are limited multilingual fact-verification datasets (Gupta and Srikumar 2021; Kazemi et al. 2022; Pikuliak et al. 2023; Singh et al. 2023). This hinders the development of ap-proaches for multilingual fact-verification, which achieve the best performance when trained on language-specific datasets (Panchendrarajan and Zubiaga 2024). 

# Conclusion 

We presented a survey on LLM approaches to claim verifi-cation. To the best of our knowledge, this is the first claim verification survey to focus exclusively on LLM approaches thus filling an important gap in the literature. We have de-scribed each of the sub-tasks of the typical claim verifica-tion pipeline and discussed various LLM-based approaches used in this task. Finally, we have also described publicly available English datasets providing important information to new and seasoned researchers on this topic. Advances in LLM development will likely continue im-proving the quality of claim verification systems. We hope this survey motivates future research on this topic taking advantage of recently-proposed LLMs, RAG methods, etc. Claim verification is a vibrant research topic and we see multiple open research directions as described in the next sub-section. References 

Adair, B.; Li, C.; Yang, J.; and Yu, C. 2017. Progress toward “the holy grail”: The continued quest to auto-mate fact-checking. In Computation+ Journalism Sympo-sium,(September) .Aly, R.; Guo, Z.; Schlichtkrull, M. S.; Thorne, J.; Vla-chos, A.; Christodoulopoulos, C.; Cocarascu, O.; and Mittal, A. 2021. FEVEROUS: Fact Extraction and VERification Over Unstructured and Structured information. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1) .Augenstein, I.; Lioma, C.; Wang, D.; Lima, L. C.; Hansen, C.; Hansen, C.; and Simonsen, J. G. 2019. MultiFC: A Real-World Multi-Domain Dataset for Evidence-Based Fact Checking of Claims. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , 4685–4697. Barr´ on-Cedeno, A.; Elsayed, T.; Nakov, P.; Da San Mar-tino, G.; Hasanain, M.; Suwaileh, R.; Haouari, F.; Babulkov, N.; Hamdan, B.; Nikolov, A.; et al. 2020. Overview of CheckThat! 2020: Automatic identification and verification of claims in social media. In Experimental IR Meets Mul-tilinguality, Multimodality, and Interaction: 11th Interna-tional Conference of the CLEF Association, CLEF 2020, Thessaloniki, Greece, September 22–25, 2020, Proceedings 11 . Springer. Bashlovkina, V.; Kuang, Z.; Matthews, R.; Clifford, E.; Jun, Y.; Cohen, W. W.; and Baumgartner, S. 2023. Trusted source alignment in large language models. arXiv preprint arXiv:2311.06697 .Bekoulis, G.; Papagiannopoulou, C.; and Deligiannis, N. 2021. A review on fact extraction and verification. ACM Computing Surveys (CSUR) , 55(1): 1–35. Beltagy, I.; Peters, M. E.; and Cohan, A. 2020. Long-former: The long-document transformer. arXiv preprint arXiv:2004.05150 .Brown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.; Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; et al. 2020. Language models are few-shot learners. Ad-vances in neural information processing systems , 33: 1877– 1901. Cao, Y.; Nair, A. M.; Eyimife, E.; Soofi, N. J.; Subbalak-shmi, K.; Wullert II, J. R.; Basu, C.; and Shallcross, D. 2024. Can Large Language Models Detect Misinformation in Sci-entific News Reporting? arXiv preprint arXiv:2402.14268 .Chen, C.; and Shu, K. 2023. Combating misinformation in the age of llms: Opportunities and challenges. arXiv preprint arXiv:2311.05656 .Chen, D.; Fisch, A.; Weston, J.; and Bordes, A. 2017. Read-ing Wikipedia to Answer Open-Domain Questions. In Pro-ceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) .Chen, H.-T.; Zhang, M.; and Choi, E. 2022. Rich Knowledge Sources Bring Complex Knowledge Conflicts: Recalibrating Models to Reflect Conflicting Evidence. In Proceedings of the 2022 Conference on Empirical Methods in Natural Lan-guage Processing .Chen, J.; Kim, G.; Sriram, A.; Durrett, G.; and Choi, E. 2023a. Complex claim verification with evidence retrieved in the wild. arXiv preprint arXiv:2305.11859 .Chen, J.; Sriram, A.; Choi, E.; and Durrett, G. 2022a. Gener-ating Literal and Implied Subquestions to Fact-check Com-plex Claims. In Proceedings of the 2022 Conference on Em-pirical Methods in Natural Language Processing .Chen, J.; Zhang, R.; Guo, J.; de Rijke, M.; Liu, Y.; Fan, Y.; and Cheng, X. 2023b. A unified generative retriever for knowledge-intensive language tasks via prompt learn-ing. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval .Chen, J.; Zhang, R.; Guo, J.; Fan, Y.; and Cheng, X. 2022b. GERE: Generative evidence retrieval for fact verification. In Proceedings of the 45th International ACM SIGIR Con-ference on Research and Development in Information Re-trieval .Chen, J.; Zhang, R.; Guo, J.; Liu, Y.; Fan, Y.; and Cheng, X. 2022c. Corpusbrain: Pre-train a generative retrieval model for knowledge-intensive language tasks. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management .Cheng, X.; Tan, Z.; and Lu, W. 2024. Information Re-Organization Improves Reasoning in Large Language Mod-els. arXiv preprint arXiv:2404.13985 .Chern, I.; Chern, S.; Chen, S.; Yuan, W.; Feng, K.; Zhou, C.; He, J.; Neubig, G.; Liu, P.; et al. 2023. FacTool: Factuality Detection in Generative AI–A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios. arXiv preprint arXiv:2307.13528 .Chiang, S.-H.; Lo, M.-C.; Chao, L.-W.; and Peng, W.-C. 2024. Team Trifecta at Factify5WQA: Setting the Stan-dard in Fact Verification with Fine-Tuning. arXiv preprint arXiv:2403.10281 .Choi, E. C.; and Ferrara, E. 2024. Automated claim match-ing with large language models: empowering fact-checkers in the fight against misinformation. In Companion Proceed-ings of the ACM on Web Conference 2024 .Dammu, P. P. S.; Naidu, H.; Dewan, M.; Kim, Y.; Roosta, T.; Chadha, A.; and Shah, C. 2024. ClaimVer: Ex-plainable Claim-Level Verification and Evidence Attribu-tion of Text Through Knowledge Graphs. arXiv preprint arXiv:2403.09724 .Diggelmann, T.; Boyd-Graber, J.; Bulian, J.; Ciaramita, M.; and Leippold, M. 2020. CLIMATE-FEVER: A Dataset for Verification of Real-World Climate Claims. Tackling Cli-mate Change with Machine Learning workshop at NeurIPS 2020 .Dmonte, A.; Zampieri, M.; Lybarger, K.; and Albanese, M. 2024. Classifying Human-Generated and AI-Generated Election Claims in Social Media. arXiv preprint arXiv:2404.16116 .Dougrez-Lewis, J.; Akhter, M. E.; He, Y.; and Liakata, M. 2024. Assessing the Reasoning Abilities of Chat-GPT in the Context of Claim Verification. arXiv preprint arXiv:2402.10735 .Eisenschlos, J.; Dhingra, B.; Bulian, J.; B¨ orschinger, B.; and Boyd-Graber, J. 2021. Fool Me Twice: Entailment from Wikipedia Gamification. In Proceedings of the 2021 Con-ference of the North American Chapter of the Association for Computational Linguistics: Human Language Technolo-gies .Fang, Y.; Li, M.; Wang, W.; Lin, H.; and Feng, F. 2024. Counterfactual Debating with Preset Stances for Hallucination Elimination of LLMs. arXiv preprint arXiv:2406.11514 .Gao, Y.; Xiong, Y.; Gao, X.; Jia, K.; Pan, J.; Bi, Y.; Dai, Y.; Sun, J.; and Wang, H. 2023. Retrieval-augmented gen-eration for large language models: A survey. arXiv preprint arXiv:2312.10997 .Glass, M.; Rossiello, G.; Chowdhury, M. F. M.; Naik, A.; Cai, P.; and Gliozzo, A. 2022. Re2G: Retrieve, Rerank, Generate. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computa-tional Linguistics: Human Language Technologies .Guan, J.; Dodge, J.; Wadden, D.; Huang, M.; and Peng, H. 2023. Language Models Hallucinate, but May Excel at Fact Verification. arXiv preprint arXiv:2310.14564 .Guo, Z.; Schlichtkrull, M.; and Vlachos, A. 2022. A survey on automated fact-checking. Transactions of the Association for Computational Linguistics , 10: 178–206. Gupta, A.; and Srikumar, V. 2021. X-Fact: A New Bench-mark Dataset for Multilingual Fact Checking. In Proceed-ings of the 59th Annual Meeting of the Association for Com-putational Linguistics and the 11th International Joint Con-ference on Natural Language Processing (Volume 2: Short Papers) .Hang, C. N.; Yu, P.-D.; and Tan, C. W. 2024. TrumorGPT: Query Optimization and Semantic Reasoning over Networks for Automated Fact-Checking. In 2024 58th Annual Confer-ence on Information Sciences and Systems (CISS) .Hanselowski, A. 2020. A machine-learning-based pipeline approach to automated fact-checking. Hassan, N.; Li, C.; and Tremayne, M. 2015. Detecting check-worthy factual claims in presidential debates. In Pro-ceedings of the 24th acm international on conference on in-formation and knowledge management .Hofst¨ atter, S.; Chen, J.; Raman, K.; and Zamani, H. 2023. Fid-light: Efficient and effective retrieval-augmented text generation. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Infor-mation Retrieval .Hu, X.; Chen, J.; Guo, Z.; and Yu, P. S. 2023. Give Me More Details: Improving Fact-Checking with Latent Re-trieval. arXiv preprint arXiv:2305.16128 .Huang, C.; Wu, Z.; Hu, Y.; and Wang, W. 2024. Training Language Models to Generate Text with Citations via Fine-grained Rewards. arXiv preprint arXiv:2402.04315 .Izacard, G.; Lewis, P.; Lomeli, M.; Hosseini, L.; Petroni, F.; Schick, T.; Dwivedi-Yu, J.; Joulin, A.; Riedel, S.; and Grave, E. 2023. Atlas: Few-shot learning with retrieval augmented language models. Journal of Machine Learning Research ,24(251): 1–43. Jafari, N.; and Allan, J. 2024. Robust Claim Verification Through Fact Detection. arXiv preprint arXiv:2407.18367 .Jiang, Y.; Bordia, S.; Zhong, Z.; Dognin, C.; Singh, M.; and Bansal, M. 2020. HoVer: A Dataset for Many-Hop Fact Ex-traction And Claim Verification. In Findings of the Associa-tion for Computational Linguistics: EMNLP 2020 .Kamoi, R.; Goyal, T.; Rodriguez, J. D.; and Durrett, G. 2023. WiCE: Real-World Entailment for Claims in Wikipedia. In 

Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing , 7561–7583. Kanaani, M. 2024. Triple-R: Automatic Reasoning for Fact Verification Using Language Models. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024) .Kao, W.-Y.; and Yen, A.-Z. 2024. MAGIC: Multi-Argument Generation with Self-Refinement for Domain Generaliza-tion in Automatic Fact-Checking. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024) .Katranidis, V.; and Barany, G. 2024. FaaF: Facts as a Func-tion for the evaluation of RAG systems. arXiv preprint arXiv:2403.03888 .Kazemi, A.; Li, Z.; Per´ ez-Rosas, V.; Hale, S.; and Mihalcea, R. 2022. Matching tweets with applicable fact-checks across languages. In CEUR Workshop Proceedings .Khattab, O.; Potts, C.; and Zaharia, M. 2021. Baleen: Robust Multi-Hop Reasoning at Scale via Condensed Retrieval. In Beygelzimer, A.; Dauphin, Y.; Liang, P.; and Vaughan, J. W., eds., Advances in Neural Information Processing Systems .Kojima, T.; Gu, S. S.; Reid, M.; Matsuo, Y.; and Iwasawa, Y. 2022. Large language models are zero-shot reason-ers. Advances in neural information processing systems , 35: 22199–22213. Lee, N.; Ping, W.; Xu, P.; Patwary, M.; Fung, P.; Shoeybi, M.; and Catanzaro, B. 2022. Factuality Enhanced Language Models for Open-Ended Text Generation. In Oh, A. H.; Agarwal, A.; Belgrave, D.; and Cho, K., eds., Advances in Neural Information Processing Systems .Lewis, P.; Perez, E.; Piktus, A.; Petroni, F.; Karpukhin, V.; Goyal, N.; K¨ uttler, H.; Lewis, M.; Yih, W.-t.; Rockt¨ aschel, T.; et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Infor-mation Processing Systems , 33: 9459–9474. Li, D.; Rawat, A. S.; Zaheer, M.; Wang, X.; Lukasik, M.; Veit, A.; Yu, F.; and Kumar, S. 2023a. Large Language Mod-els with Controllable Working Memory. In Findings of the Association for Computational Linguistics: ACL 2023 .Li, J.; Zhao, R.; Yang, Y.; He, Y.; and Gui, L. 2023b. Over-Prompt: Enhancing ChatGPT through Efficient In-Context Learning. In R0-FoMo:Robustness of Few-shot and Zero-shot Learning in Large Foundation Models .Li, M.; Peng, B.; Galley, M.; Gao, J.; and Zhang, Z. 2023c. Self-checker: Plug-and-play modules for fact-checking with large language models. arXiv preprint arXiv:2305.14623 .Li, X.; Chen, S.; Kapadia, R.; Ouyang, J.; and Zhang, F. 2024a. Minimal Evidence Group Identification for Claim Verification. arXiv preprint arXiv:2404.15588 .Li, X.; Zhao, R.; Chia, Y. K.; Ding, B.; Joty, S.; Poria, S.; and Bing, L. 2024b. Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources. In The Twelfth International Con-ference on Learning Representations .Liu, H.; Soroush, A.; Nestor, J. G.; Park, E.; Idnay, B.; Fang, Y.; Pan, J.; Liao, S.; Bernard, M.; Peng, Y.; et al. 2024. Re-trieval augmented scientific claim verification. JAMIA open ,7(1): ooae021. Longpre, S.; Perisetla, K.; Chen, A.; Ramesh, N.; DuBois, C.; and Singh, S. 2021. Entity-Based Knowledge Conflicts in Question Answering. In Proceedings of the 2021 Confer-ence on Empirical Methods in Natural Language Process-ing .Lu, X.; Pan, L.; Liu, Q.; Nakov, P.; and Kan, M.-Y. 2023. SCITAB: A Challenging Benchmark for Compositional Reasoning and Claim Verification on Scientific Tables. In 

Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing .Ma, H.; Xu, W.; Wei, Y.; Chen, L.; Wang, L.; Liu, Q.; and Wu, S. 2024. EX-FEVER: A Dataset for Multi-hop Explain-able Fact Verification. In Findings of the Association for Computational Linguistics: ACL 2024 .Malaviya, C.; Lee, S.; Chen, S.; Sieber, E.; Yatskar, M.; and Roth, D. 2024. ExpertQA: Expert-Curated Questions and Attributed Answers. In Proceedings of the 2024 Confer-ence of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers) , 3025–3045. Mallen, A.; Asai, A.; Zhong, V.; Das, R.; Khashabi, D.; and Hajishirzi, H. 2023. When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) .Min, S.; Krishna, K.; Lyu, X.; Lewis, M.; Yih, W.-t.; Koh, P.; Iyyer, M.; Zettlemoyer, L.; and Hajishirzi, H. 2023. FActScore: Fine-grained Atomic Evaluation of Factual Pre-cision in Long Form Text Generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Lan-guage Processing , 12076–12100. Misra, R. 2022. Politifact Fact Check Dataset. Nakov, P.; Da San Martino, G.; Elsayed, T.; Barr´ on-Cede˜ no, A.; M´ ıguez, R.; Shaar, S.; Alam, F.; Haouari, F.; Hasanain, M.; Babulkov, N.; et al. 2021. The CLEF-2021 CheckThat! Lab on Detecting Check-Worthy Claims, Previously Fact-Checked Claims, and Fake News. In European Conference on Information Retrieval .Neeman, E.; Aharoni, R.; Honovich, O.; Choshen, L.; Szpektor, I.; and Abend, O. 2023. DisentQA: Disentangling Parametric and Contextual Knowledge with Counterfactual Question Answering. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) .Ni, J.; Shi, M.; Stammbach, D.; Sachan, M.; Ash, E.; and Leippold, M. 2024. AFaCTA: Assisting the Annotation of Factual Claim Detection with Reliable LLM Annotators. 

arXiv preprint arXiv:2402.11073 .Pan, L.; Chen, W.; Xiong, W.; Kan, M.-Y.; and Wang, W. Y. 2021. Zero-shot Fact Verification by Claim Generation. Pro-ceedings of ACL 2021 .Pan, L.; Lu, X.; Kan, M.-Y.; and Nakov, P. 2023a. QACheck: A Demonstration System for Question-Guided Multi-Hop Fact-Checking. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: Sys-tem Demonstrations , 264–273. Pan, L.; Wu, X.; Lu, X.; Luu, A. T.; Wang, W. Y.; Kan, M.-Y.; and Nakov, P. 2023b. Fact-Checking Complex Claims with Program-Guided Reasoning. In Proceedings of the 61st Annual Meeting of the Association for Computational Lin-guistics (Volume 1: Long Papers) .Pan, Y.; Pan, L.; Chen, W.; Nakov, P.; Kan, M.-Y.; and Wang, W. 2023c. On the Risk of Misinformation Pollution with Large Language Models. In Findings of the Association for Computational Linguistics: EMNLP 2023 .Panchendrarajan, R.; and Zubiaga, A. 2024. Claim detec-tion for automated fact-checking: A survey on monolingual, multilingual and cross-lingual research. Natural Language Processing Journal , 7: 100066. Park, J.; Min, S.; Kang, J.; Zettlemoyer, L.; and Hajishirzi, H. 2022. FaVIQ: FAct Verification from Information-seeking Questions. In Proceedings of the 60th Annual Meet-ing of the Association for Computational Linguistics (Vol-ume 1: Long Papers) , 5154–5166. Parvez, M. R. 2024. Evidence to Generate (E2G): A Single-agent Two-step Prompting for Context Grounded and Retrieval Augmented Reasoning. arXiv preprint arXiv:2401.05787 .Petroni, F.; Lewis, P.; Piktus, A.; Rockt¨ aschel, T.; Wu, Y.; Miller, A. H.; and Riedel, S. 2020. How Context Af-fects Language Models’ Factual Predictions. In Automated Knowledge Base Construction .Pikuliak, M.; Srba, I.; Moro, R.; Hromadka, T.; Smoleˇ n, T.; Meliˇ sek, M.; Vykopal, I.; Simko, J.; Podrouˇ zek, J.; and Bielikov´ a, M. 2023. Multilingual Previously Fact-Checked Claim Retrieval. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing .Pradeep, R.; Ma, X.; Nogueira, R.; and Lin, J. 2021. Scien-tific Claim Verification with VerT5erini. In Proceedings of the 12th International Workshop on Health Text Mining and Information Analysis .Quelle, D.; and Bovet, A. 2024. The perils and promises of fact-checking with large language models. Frontiers in Artificial Intelligence , 7: 1341697. Rae, J. W.; Borgeaud, S.; Cai, T.; Millican, K.; Hoff-mann, J.; Song, F.; Aslanides, J.; Henderson, S.; Ring, R.; Young, S.; et al. 2021. Scaling language models: Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446 .Rani, A.; Tonmoy, S. T. I.; Dalal, D.; Gautam, S.; Chakraborty, M.; Chadha, A.; Sheth, A.; and Das, A. 2023. FACTIFY-5WQA: 5W Aspect-based Fact Verifica-tion through Question Answering. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) .Saakyan, A.; Chakrabarty, T.; and Muresan, S. 2021. COVID-Fact: Fact Extraction and Verification of Real-World Claims on COVID-19 Pandemic. In Proceedings of the 59th Annual Meeting of the Association for Computa-tional Linguistics and the 11th International Joint Confer-ence on Natural Language Processing (Volume 1: Long Pa-pers) .Sarrouti, M.; Abacha, A. B.; M’rabet, Y.; and Demner-Fushman, D. 2021. Evidence-based fact-checking of health-related claims. In Findings of the Association for Computa-tional Linguistics: EMNLP 2021 .Schlichtkrull, M.; Guo, Z.; and Vlachos, A. 2024. Averitec: A dataset for real-world claim verification with evidence from the web. Advances in Neural Information Processing Systems , 36. Schuster, T.; Fisch, A.; and Barzilay, R. 2021. Get Your Vita-min C! Robust Fact Verification with Contrastive Evidence. In Proceedings of the 2021 Conference of the North Ameri-can Chapter of the Association for Computational Linguis-tics: Human Language Technologies , 624–643. Shaar, S.; Babulkov, N.; Da San Martino, G.; and Nakov, P. 2020. That is a Known Lie: Detecting Previously Fact-Checked Claims. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics .Shao, Z.; Gong, Y.; Shen, Y.; Huang, M.; Duan, N.; and Chen, W. 2023. Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Syn-ergy. In Findings of the Association for Computational Lin-guistics: EMNLP 2023 .Shi, F.; Chen, X.; Misra, K.; Scales, N.; Dohan, D.; Chi, E. H.; Sch¨ arli, N.; and Zhou, D. 2023. Large language mod-els can be easily distracted by irrelevant context. In Interna-tional Conference on Machine Learning .Si, C.; Goyal, N.; Wu, S. T.; Zhao, C.; Feng, S.; Daum´ e III, H.; and Boyd-Graber, J. 2023. Large Language Models Help Humans Verify Truthfulness–Except When They Are Con-vincingly Wrong. arXiv preprint arXiv:2310.12558 .Singh, I.; Scarton, C.; Song, X.; and Bontcheva, K. 2023. Finding Already Debunked Narratives via Multistage Re-trieval: Enabling Cross-Lingual, Cross-Dataset and Zero-Shot Learning. arXiv preprint arXiv:2308.05680 .Song, Y.; Kim, Y.; and Iyyer, M. 2024. VERISCORE: Eval-uating the factuality of verifiable claims in long-form text generation. arXiv preprint arXiv:2406.19276 .Tang, L.; Laban, P.; and Durrett, G. 2024. MiniCheck: Ef-ficient Fact-Checking of LLMs on Grounding Documents. 

arXiv preprint arXiv:2404.10774 .Thorne, J.; Vlachos, A.; Christodoulopoulos, C.; and Mittal, A. 2018a. FEVER: a Large-scale Dataset for Fact Extrac-tion and VERification. In Proceedings of the 2018 Confer-ence of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers) .Thorne, J.; Vlachos, A.; Cocarascu, O.; Christodoulopoulos, C.; and Mittal, A. 2018b. The Fact Extraction and VERifi-cation (FEVER) Shared Task. In Proceedings of the First Workshop on Fact Extraction and VERification (FEVER) .Vallayil, M.; Nand, P.; Yan, W. Q.; and Allende-Cid, H. 2023. Explainability of automated fact verification systems: A comprehensive review. Applied Sciences , 13(23): 12608. Venktesh, V.; Anand, A.; Anand, A.; and Setty, V. 2024. QuanTemp: A real-world open-domain bench-mark for fact-checking numerical claims. arXiv preprint arxiv:2403.17169 .Vladika, J.; and Matthes, F. 2023. Scientific Fact-Checking: A Survey of Resources and Approaches. In Findings of the Association for Computational Linguistics: ACL 2023 .Wadden, D.; Lin, S.; Lo, K.; Wang, L. L.; van Zuylen, M.; Cohan, A.; and Hajishirzi, H. 2020. Fact or Fiction: Veri-fying Scientific Claims. In Proceedings of the 2020 Confer-ence on Empirical Methods in Natural Language Processing (EMNLP) .Wadden, D.; and Lo, K. 2021. Overview and Insights from the SCIVER shared task on Scientific Claim Verification. In Proceedings of the Second Workshop on Scholarly Docu-ment Processing .Wadden, D.; Lo, K.; Kuehl, B.; Cohan, A.; Beltagy, I.; Wang, L. L.; and Hajishirzi, H. 2022a. SciFact-Open: To-wards open-domain scientific claim verification. In Findings of the Association for Computational Linguistics: EMNLP 2022 , 4719–4734. Wadden, D.; Lo, K.; Wang, L. L.; Cohan, A.; Beltagy, I.; and Hajishirzi, H. 2022b. MultiVerS: Improving scientific claim verification with weak supervision and full-document context. In Findings of the Association for Computational Linguistics: NAACL 2022 , 61–76. Wang, G.; Harwood, K.; Chillrud, L.; Ananthram, A.; Sub-biah, M.; and Mckeown, K. 2023a. Check-COVID: Fact-Checking COVID-19 News Claims with Scientific Evi-dence. In Findings of the Association for Computational Linguistics: ACL 2023 , 14114–14127. Wang, H.; and Shu, K. 2023. Explainable Claim Verification via Knowledge-Grounded Reasoning with Large Language Models. In Findings of the Association for Computational Linguistics: EMNLP 2023 .Wang, N. X.; Mahajan, D.; Danilevsky, M.; and Rosenthal, S. 2021. SemEval-2021 Task 9: Fact Verification and Ev-idence Finding for Tabular Data in Scientific Documents (SEM-TAB-FACTS). In Proceedings of the 15th Interna-tional Workshop on Semantic Evaluation (SemEval-2021) .Wang, W. Y. 2017. “Liar, Liar Pants on Fire”: A New Bench-mark Dataset for Fake News Detection. In Proceedings of the 55th Annual Meeting of the Association for Computa-tional Linguistics (Volume 2: Short Papers) , 422–426. Wang, Y.; Reddy, R. G.; Mujahid, Z. M.; Arora, A.; Ruba-shevskii, A.; Geng, J.; Afzal, O. M.; Pan, L.; Borenstein, N.; Pillai, A.; Augenstein, I.; Gurevych, I.; and Nakov, P. 2023b. Factcheck-Bench: Fine-Grained Evaluation Benchmark for Automatic Fact-checkers. Wang, Z.; Araki, J.; Jiang, Z.; Parvez, M. R.; and Neubig, G. 2023c. Learning to filter context for retrieval-augmented generation. arXiv preprint arXiv:2311.08377 .Wei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Xia, F.; Chi, E.; Le, Q. V.; Zhou, D.; et al. 2022. Chain-of-thought prompting elicits reasoning in large language mod-els. Advances in neural information processing systems , 35: 24824–24837. Wei, J.; Yang, C.; Song, X.; Lu, Y.; Hu, N.; Tran, D.; Peng, D.; Liu, R.; Huang, D.; Du, C.; et al. 2024. Long-form factuality in large language models. arXiv preprint arXiv:2403.18802 .Xia, Y.; Zhou, J.; Shi, Z.; Chen, J.; and Huang, H. 2024. Improving Retrieval Augmented Language Model with Self-Reasoning. arXiv preprint arXiv:2407.19813 .Xu, R.; Qi, Z.; Wang, C.; Wang, H.; Zhang, Y.; and Xu, W. 2024a. Knowledge Conflicts for LLMs: A Survey. arXiv preprint arXiv:2403.08319 .Xu, S.; Pang, L.; Shen, H.; Cheng, X.; and Chua, T.-S. 2024b. Search-in-the-Chain: Interactively Enhancing Large Language Models with Search for Knowledge-intensive Tasks. In Proceedings of the ACM on Web Conference 2024 ,1362–1373. Yang, Z.; Ma, J.; Chen, H.; Lin, H.; Luo, Z.; and Chang, Y. 2022. A Coarse-to-fine Cascaded Evidence-Distillation Neural Network for Explainable Fake News Detection. In 

Proceedings of the 29th International Conference on Com-putational Linguistics .Yao, S.; Zhao, J.; Yu, D.; Du, N.; Shafran, I.; Narasimhan, K. R.; and Cao, Y. 2023. ReAct: Synergizing Reasoning and Acting in Language Models. In The Eleventh International Conference on Learning Representations .Yoran, O.; Wolfson, T.; Ram, O.; and Berant, J. 2024. Mak-ing Retrieval-Augmented Language Models Robust to Irrel-evant Context. In The Twelfth International Conference on Learning Representations .Yu, W.; Zhang, H.; Pan, X.; Ma, K.; Wang, H.; and Yu, D. 2023. Chain-of-note: Enhancing robustness in retrieval-augmented language models. arXiv preprint arXiv:2311.09210 .Zamani, H.; and Bendersky, M. 2024. Stochastic RAG: End-to-End Retrieval-Augmented Generation through Expected Utility Maximization. In Proceedings of the 47th Interna-tional ACM SIGIR Conference on Research and Develop-ment in Information Retrieval , 2641–2646. Zelikman, E.; Wu, Y.; Mu, J.; and Goodman, N. 2022. Star: Bootstrapping reasoning with reasoning. Advances in Neu-ral Information Processing Systems , 35: 15476–15488. Zeng, F.; and Gao, W. 2023. Prompt to be Consistent is Bet-ter than Self-Consistent? Few-Shot and Zero-Shot Fact Ver-ification with Pre-trained Language Models. In Findings of the Association for Computational Linguistics: ACL 2023 .Zeng, X.; Abumansour, A. S.; and Zubiaga, A. 2021. Au-tomated fact-checking: A survey. Language and Linguistics Compass , 15(10): e12438. Zeng, X.; and Zubiaga, A. 2024. MAPLE: Micro Analysis of Pairwise Language Evolution for Few-Shot Claim Verifi-cation. arXiv preprint arXiv:2401.16282 .Zha, Y.; Yang, Y.; Li, R.; and Hu, Z. 2023. AlignScore: Evaluating Factual Consistency with A Unified Alignment Function. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , 11328–11348. Zhang, H.; Zhang, R.; Guo, J.; de Rijke, M.; Fan, Y.; and Cheng, X. 2023. From Relevance to Utility: Evidence Re-trieval with Feedback for Fact Verification. In Findings of the Association for Computational Linguistics: EMNLP 2023 , 6373–6384. Zhang, X.; and Gao, W. 2023. Towards LLM-based Fact Verification on News Claims with a Hierarchical Step-by-Step Prompting Method. In Proceedings of the 13th Inter-national Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (Volume 1: Long Papers) .Zhang, X.; and Gao, W. 2024. Reinforcement Retrieval Leveraging Fine-grained Feedback for Fact Checking News Claims with Black-Box LLM. In Proceedings of the 2024 Joint International Conference on Computational Linguis-tics, Language Resources and Evaluation (LREC-COLING 2024) , 13861–13873. Zhao, X.; Wang, L.; Wang, Z.; Cheng, H.; Zhang, R.; and Wong, K.-F. 2024. Pacar: Automated fact-checking with planning and customized action reasoning using large lan-guage models. In Proceedings of the 2024 Joint Interna-tional Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024) .Zhou, J.; Zhang, Y.; Luo, Q.; Parker, A. G.; and De Choud-hury, M. 2023. Synthetic lies: Understanding ai-generated misinformation and evaluating algorithmic and human solu-tions. In Proceedings of the 2023 CHI Conference on Hu-man Factors in Computing Systems .Zhu, Y.; Si, J.; Zhao, Y.; Zhu, H.; Zhou, D.; and He, Y. 2023. EXPLAIN, EDIT, GENERATE: Rationale-Sensitive Coun-terfactual Data Augmentation for Multi-hop Fact Verifica-tion. In The 2023 Conference on Empirical Methods in Nat-ural Language Processing .
